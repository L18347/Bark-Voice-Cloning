{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/L18347/Bark-Voice-Cloning/blob/main/notebooks/GPT_SoVITS_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 一键启动！\n",
        "!git clone https://github.com/KevinWang676/GPT-SoVITS-v2.git\n",
        "!git clone https://huggingface.co/spaces/kevinwang676/GPT-SoVITS-models.git\n",
        "%cd GPT-SoVITS-v2\n",
        "!pip install --disable-pip-version-check -r requirements.txt\n",
        "!sudo apt install ffmpeg\n",
        "!sudo apt install libsox-dev\n",
        "!mv /content/GPT-SoVITS-models/GPT-SoVITS/tools/damo_asr/models /content/GPT-SoVITS-v2/tools/damo_asr\n",
        "!mv /content/GPT-SoVITS-models/GPT-SoVITS/GPT_SoVITS/pretrained_models /content/GPT-SoVITS-v2/GPT_SoVITS\n",
        "import nltk\n",
        "nltk.download(\"all\")\n",
        "!python webui.py"
      ],
      "metadata": {
        "id": "WXqXTJnmq4t6",
        "outputId": "dd7c4b97-9637-4980-97e6-366d63d1416e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'GPT-SoVITS-v2' already exists and is not an empty directory.\n",
            "fatal: destination path 'GPT-SoVITS-models' already exists and is not an empty directory.\n",
            "/content/GPT-SoVITS-v2\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.11.4)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.15.2)\n",
            "Requirement already satisfied: librosa==0.9.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.9.2)\n",
            "Requirement already satisfied: numba==0.56.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.56.4)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.2.5)\n",
            "Requirement already satisfied: gradio==3.38.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (3.38.0)\n",
            "Requirement already satisfied: gradio_client==0.8.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (0.8.1)\n",
            "Requirement already satisfied: ffmpeg-python in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (0.2.0)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (1.18.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (4.66.4)\n",
            "Requirement already satisfied: funasr==1.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (1.0.0)\n",
            "Requirement already satisfied: cn2an in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (0.5.22)\n",
            "Requirement already satisfied: pypinyin in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (0.51.0)\n",
            "Requirement already satisfied: pyopenjtalk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (0.3.3)\n",
            "Requirement already satisfied: g2p_en in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (2.1.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (2.3.0+cu121)\n",
            "Requirement already satisfied: modelscope==1.10.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (1.10.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (0.1.99)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 20)) (4.41.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (5.2.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (6.0.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 23)) (5.9.5)\n",
            "Requirement already satisfied: jieba_fast in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 24)) (0.53)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 25)) (0.42.1)\n",
            "Requirement already satisfied: LangSegment>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 26)) (0.3.3)\n",
            "Requirement already satisfied: Faster_Whisper in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 27)) (1.0.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (4.4.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (0.4.3)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (1.8.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (24.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba==0.56.4->-r requirements.txt (line 5)) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba==0.56.4->-r requirements.txt (line 5)) (67.7.2)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.38.0->-r requirements.txt (line 7)) (23.2.1)\n",
            "Requirement already satisfied: aiohttp~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.38.0->-r requirements.txt (line 7)) (3.9.5)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.38.0->-r requirements.txt (line 7)) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio==3.38.0->-r requirements.txt (line 7)) (0.111.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio==3.38.0->-r requirements.txt (line 7)) (0.3.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio==3.38.0->-r requirements.txt (line 7)) (0.27.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.38.0->-r requirements.txt (line 7)) (0.23.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.38.0->-r requirements.txt (line 7)) (3.1.4)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.38.0->-r requirements.txt (line 7)) (2.2.0)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.38.0->-r requirements.txt (line 7)) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.38.0->-r requirements.txt (line 7)) (3.7.1)\n",
            "Requirement already satisfied: mdit-py-plugins<=0.3.3 in /usr/local/lib/python3.10/dist-packages (from gradio==3.38.0->-r requirements.txt (line 7)) (0.3.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.38.0->-r requirements.txt (line 7)) (3.10.3)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.38.0->-r requirements.txt (line 7)) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.38.0->-r requirements.txt (line 7)) (9.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from gradio==3.38.0->-r requirements.txt (line 7)) (2.7.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio==3.38.0->-r requirements.txt (line 7)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio==3.38.0->-r requirements.txt (line 7)) (0.0.9)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.38.0->-r requirements.txt (line 7)) (2.31.0)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.38.0->-r requirements.txt (line 7)) (2.10.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.38.0->-r requirements.txt (line 7)) (4.11.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.38.0->-r requirements.txt (line 7)) (0.29.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.38.0->-r requirements.txt (line 7)) (11.0.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio_client==0.8.1->-r requirements.txt (line 8)) (2023.6.0)\n",
            "Requirement already satisfied: jamo in /usr/local/lib/python3.10/dist-packages (from funasr==1.0.0->-r requirements.txt (line 12)) (0.4.1)\n",
            "Requirement already satisfied: kaldiio>=2.17.0 in /usr/local/lib/python3.10/dist-packages (from funasr==1.0.0->-r requirements.txt (line 12)) (2.18.0)\n",
            "Requirement already satisfied: torch-complex in /usr/local/lib/python3.10/dist-packages (from funasr==1.0.0->-r requirements.txt (line 12)) (0.4.3)\n",
            "Requirement already satisfied: pytorch-wpe in /usr/local/lib/python3.10/dist-packages (from funasr==1.0.0->-r requirements.txt (line 12)) (0.0.1)\n",
            "Requirement already satisfied: editdistance>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from funasr==1.0.0->-r requirements.txt (line 12)) (0.6.2)\n",
            "Requirement already satisfied: oss2 in /usr/local/lib/python3.10/dist-packages (from funasr==1.0.0->-r requirements.txt (line 12)) (2.18.5)\n",
            "Requirement already satisfied: hdbscan in /usr/local/lib/python3.10/dist-packages (from funasr==1.0.0->-r requirements.txt (line 12)) (0.8.33)\n",
            "Requirement already satisfied: umap in /usr/local/lib/python3.10/dist-packages (from funasr==1.0.0->-r requirements.txt (line 12)) (0.1.1)\n",
            "Requirement already satisfied: jaconv in /usr/local/lib/python3.10/dist-packages (from funasr==1.0.0->-r requirements.txt (line 12)) (0.3.4)\n",
            "Requirement already satisfied: hydra-core>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from funasr==1.0.0->-r requirements.txt (line 12)) (1.3.2)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (2.4.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (23.2.0)\n",
            "Requirement already satisfied: datasets>=2.14.5 in /usr/local/lib/python3.10/dist-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (2.19.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (0.8.0)\n",
            "Requirement already satisfied: filelock>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (3.14.0)\n",
            "Requirement already satisfied: gast>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (0.5.4)\n",
            "Requirement already satisfied: pyarrow!=9.0.0,>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (14.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (2.8.2)\n",
            "Requirement already satisfied: simplejson>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (3.19.2)\n",
            "Requirement already satisfied: sortedcontainers>=1.5.9 in /usr/local/lib/python3.10/dist-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (2.4.0)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.10/dist-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (2.0.7)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.10/dist-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (0.40.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 3)) (1.63.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 3)) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 3)) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 3)) (3.20.3)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 3)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 3)) (3.0.3)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning->-r requirements.txt (line 6)) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning->-r requirements.txt (line 6)) (1.4.0.post0)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning->-r requirements.txt (line 6)) (0.11.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python->-r requirements.txt (line 9)) (0.18.3)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime->-r requirements.txt (line 10)) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime->-r requirements.txt (line 10)) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime->-r requirements.txt (line 10)) (1.12)\n",
            "Requirement already satisfied: proces>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from cn2an->-r requirements.txt (line 13)) (0.1.7)\n",
            "Requirement already satisfied: nltk>=3.2.4 in /usr/local/lib/python3.10/dist-packages (from g2p_en->-r requirements.txt (line 16)) (3.8.1)\n",
            "Requirement already satisfied: inflect>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from g2p_en->-r requirements.txt (line 16)) (7.0.0)\n",
            "Requirement already satisfied: distance>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from g2p_en->-r requirements.txt (line 16)) (0.1.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning->-r requirements.txt (line 6)) (3.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning->-r requirements.txt (line 6)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning->-r requirements.txt (line 6)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning->-r requirements.txt (line 6)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning->-r requirements.txt (line 6)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning->-r requirements.txt (line 6)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning->-r requirements.txt (line 6)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning->-r requirements.txt (line 6)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning->-r requirements.txt (line 6)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning->-r requirements.txt (line 6)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning->-r requirements.txt (line 6)) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning->-r requirements.txt (line 6)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning->-r requirements.txt (line 6)) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->pytorch-lightning->-r requirements.txt (line 6)) (12.5.40)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 20)) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 20)) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 20)) (0.4.3)\n",
            "Requirement already satisfied: py3langid>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from LangSegment>=0.2.0->-r requirements.txt (line 26)) (0.2.2)\n",
            "Requirement already satisfied: av<13,>=11.0 in /usr/local/lib/python3.10/dist-packages (from Faster_Whisper->-r requirements.txt (line 27)) (12.0.0)\n",
            "Requirement already satisfied: ctranslate2<5,>=4.0 in /usr/local/lib/python3.10/dist-packages (from Faster_Whisper->-r requirements.txt (line 27)) (4.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio==3.38.0->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio==3.38.0->-r requirements.txt (line 7)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio==3.38.0->-r requirements.txt (line 7)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio==3.38.0->-r requirements.txt (line 7)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.0->gradio==3.38.0->-r requirements.txt (line 7)) (4.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.38.0->-r requirements.txt (line 7)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.38.0->-r requirements.txt (line 7)) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.38.0->-r requirements.txt (line 7)) (0.12.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18)) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18)) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18)) (0.70.16)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 3)) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 3)) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 3)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.3.2->funasr==1.0.0->-r requirements.txt (line 12)) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.3.2->funasr==1.0.0->-r requirements.txt (line 12)) (4.9.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.38.0->-r requirements.txt (line 7)) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.38.0->-r requirements.txt (line 7)) (2.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.38.0->-r requirements.txt (line 7)) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.38.0->-r requirements.txt (line 7)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.38.0->-r requirements.txt (line 7)) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.38.0->-r requirements.txt (line 7)) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.38.0->-r requirements.txt (line 7)) (3.1.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.4->g2p_en->-r requirements.txt (line 16)) (8.1.7)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio==3.38.0->-r requirements.txt (line 7)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio==3.38.0->-r requirements.txt (line 7)) (2024.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa==0.9.2->-r requirements.txt (line 4)) (4.2.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.38.0->-r requirements.txt (line 7)) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4->gradio==3.38.0->-r requirements.txt (line 7)) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio==3.38.0->-r requirements.txt (line 7)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio==3.38.0->-r requirements.txt (line 7)) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio==3.38.0->-r requirements.txt (line 7)) (2024.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->librosa==0.9.2->-r requirements.txt (line 4)) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.10.2->librosa==0.9.2->-r requirements.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio==3.38.0->-r requirements.txt (line 7)) (0.14.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime->-r requirements.txt (line 10)) (10.0)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio==3.38.0->-r requirements.txt (line 7)) (0.37.2)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio==3.38.0->-r requirements.txt (line 7)) (0.0.4)\n",
            "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio==3.38.0->-r requirements.txt (line 7)) (5.10.0)\n",
            "Requirement already satisfied: email_validator>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio==3.38.0->-r requirements.txt (line 7)) (2.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.38.0->-r requirements.txt (line 7)) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.38.0->-r requirements.txt (line 7)) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.38.0->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: cython<3,>=0.27 in /usr/local/lib/python3.10/dist-packages (from hdbscan->funasr==1.0.0->-r requirements.txt (line 12)) (0.29.37)\n",
            "Requirement already satisfied: crcmod>=1.7 in /usr/local/lib/python3.10/dist-packages (from oss2->funasr==1.0.0->-r requirements.txt (line 12)) (1.7)\n",
            "Requirement already satisfied: pycryptodome>=3.4.7 in /usr/local/lib/python3.10/dist-packages (from oss2->funasr==1.0.0->-r requirements.txt (line 12)) (3.20.0)\n",
            "Requirement already satisfied: aliyun-python-sdk-kms>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from oss2->funasr==1.0.0->-r requirements.txt (line 12)) (2.16.3)\n",
            "Requirement already satisfied: aliyun-python-sdk-core>=2.13.12 in /usr/local/lib/python3.10/dist-packages (from oss2->funasr==1.0.0->-r requirements.txt (line 12)) (2.15.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime->-r requirements.txt (line 10)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf->modelscope==1.10.0->-r requirements.txt (line 18)) (7.1.0)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->modelscope==1.10.0->-r requirements.txt (line 18)) (2.0.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.9.3 in /usr/local/lib/python3.10/dist-packages (from aliyun-python-sdk-core>=2.13.12->oss2->funasr==1.0.0->-r requirements.txt (line 12)) (0.10.0)\n",
            "Requirement already satisfied: cryptography>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from aliyun-python-sdk-core>=2.13.12->oss2->funasr==1.0.0->-r requirements.txt (line 12)) (42.0.7)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.9.2->-r requirements.txt (line 4)) (2.22)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi->gradio==3.38.0->-r requirements.txt (line 7)) (2.6.1)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.10/dist-packages (from fastapi-cli>=0.0.2->fastapi->gradio==3.38.0->-r requirements.txt (line 7)) (0.12.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->modelscope==1.10.0->-r requirements.txt (line 18)) (3.18.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.38.0->-r requirements.txt (line 7)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.38.0->-r requirements.txt (line 7)) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.38.0->-r requirements.txt (line 7)) (0.18.1)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio==3.38.0->-r requirements.txt (line 7)) (1.0.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 3)) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 3)) (3.2.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->gradio==3.38.0->-r requirements.txt (line 7)) (1.2.1)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio==3.38.0->-r requirements.txt (line 7)) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio==3.38.0->-r requirements.txt (line 7)) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio==3.38.0->-r requirements.txt (line 7)) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio==3.38.0->-r requirements.txt (line 7)) (0.21.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->gradio==3.38.0->-r requirements.txt (line 7)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->gradio==3.38.0->-r requirements.txt (line 7)) (13.7.1)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->gradio==3.38.0->-r requirements.txt (line 7)) (2.16.1)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libsox-dev is already the newest version (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "mv: cannot stat '/content/GPT-SoVITS-models/GPT-SoVITS/tools/damo_asr/models': No such file or directory\n",
            "mv: cannot stat '/content/GPT-SoVITS-models/GPT-SoVITS/GPT_SoVITS/pretrained_models': No such file or directory\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMPORTANT: You are using gradio version 3.38.0, however version 4.29.0 is available, please upgrade.\n",
            "--------\n",
            "Running on local URL:  http://0.0.0.0:9874\n",
            "Running on public URL: https://c610e7e9009d2d2e7b.gradio.live\n",
            "\"/usr/bin/python3\" tools/slice_audio.py \"CXY.MP3\" \"output/slicer_opt\" -34 4000 300 10 500 0.9 0.25 0 1\n",
            "执行完毕，请检查输出文件\n",
            "\"/usr/bin/python3\" tools/asr/funasr_asr.py -i \"output/slicer_opt\" -o \"output/asr_opt\" -s large -l zh -p float16\n",
            "Please install rotary_embedding_torch by: \n",
            " pip install -U rotary_embedding_torch\n",
            "Please install rotary_embedding_torch by: \n",
            " pip install -U rotary_embedding_torch\n",
            "Please install rotary_embedding_torch by: \n",
            " pip install -U rotary_embedding_torch\n",
            "Please install rotary_embedding_torch by: \n",
            " pip install -U rotary_embedding_torch\n",
            "\n",
            "tables: \n",
            "\n",
            "-----------    ** dataset_classes **    --------------\n",
            "| class name   | class location                               |\n",
            "| AudioDataset | funasr/datasets/audio_datasets/datasets.py:7 |\n",
            "-----------    ** index_ds_classes **    --------------\n",
            "| class name   | class location                               |\n",
            "| IndexDSJsonl | funasr/datasets/audio_datasets/index_ds.py:9 |\n",
            "-----------    ** batch_sampler_classes **    --------------\n",
            "| class name   | class location                               |\n",
            "| BatchSampler | funasr/datasets/audio_datasets/samplers.py:7 |\n",
            "-----------    ** frontend_classes **    --------------\n",
            "| class name        | class location                       |\n",
            "| WavFrontend       | funasr/frontends/wav_frontend.py:78  |\n",
            "| WavFrontendOnline | funasr/frontends/wav_frontend.py:216 |\n",
            "-----------    ** encoder_classes **    --------------\n",
            "| class name            | class location                                        |\n",
            "| BranchformerEncoder   | funasr/models/branchformer/encoder.py:294             |\n",
            "| ConformerChunkEncoder | funasr/models/bat/conformer_chunk_encoder.py:315      |\n",
            "| ConformerEncoder      | funasr/models/conformer/encoder.py:286                |\n",
            "| DFSMN                 | funasr/models/fsmn_vad_streaming/encoder.py:232       |\n",
            "| EBranchformerEncoder  | funasr/models/e_branchformer/encoder.py:177           |\n",
            "| FSMN                  | funasr/models/fsmn_vad_streaming/encoder.py:161       |\n",
            "| SANMEncoder           | funasr/models/sanm/encoder.py:161                     |\n",
            "| SANMEncoderChunkOpt   | funasr/models/scama/encoder.py:162                    |\n",
            "| SANMVadEncoder        | funasr/models/ct_transformer_streaming/encoder.py:148 |\n",
            "| TransformerEncoder    | funasr/models/transformer/encoder.py:139              |\n",
            "-----------    ** predictor_classes **    --------------\n",
            "| class name     | class location                                     |\n",
            "| CifPredictor   | funasr/models/paraformer/cif_predictor.py:15       |\n",
            "| CifPredictorV2 | funasr/models/paraformer/cif_predictor.py:141      |\n",
            "| CifPredictorV3 | funasr/models/bicif_paraformer/cif_predictor.py:95 |\n",
            "-----------    ** model_classes **    --------------\n",
            "| class name             | class location                                     |\n",
            "| BiCifParaformer        | funasr/models/bicif_paraformer/model.py:37         |\n",
            "| Branchformer           | funasr/models/branchformer/model.py:6              |\n",
            "| CAMPPlus               | funasr/models/campplus/model.py:30                 |\n",
            "| CTTransformer          | funasr/models/ct_transformer/model.py:30           |\n",
            "| CTTransformerStreaming | funasr/models/ct_transformer_streaming/model.py:27 |\n",
            "| Conformer              | funasr/models/conformer/model.py:8                 |\n",
            "| ContextualParaformer   | funasr/models/contextual_paraformer/model.py:43    |\n",
            "| EBranchformer          | funasr/models/e_branchformer/model.py:6            |\n",
            "| Emotion2vec            | funasr/models/emotion2vec/model.py:34              |\n",
            "| FsmnVADStreaming       | funasr/models/fsmn_vad_streaming/model.py:267      |\n",
            "| MonotonicAligner       | funasr/models/monotonic_aligner/model.py:24        |\n",
            "| Paraformer             | funasr/models/paraformer/model.py:26               |\n",
            "| ParaformerStreaming    | funasr/models/paraformer_streaming/model.py:37     |\n",
            "| SANM                   | funasr/models/sanm/model.py:13                     |\n",
            "| SCAMA                  | funasr/models/scama/model.py:38                    |\n",
            "| SeacoParaformer        | funasr/models/seaco_paraformer/model.py:45         |\n",
            "| Transformer            | funasr/models/transformer/model.py:20              |\n",
            "| UniASR                 | funasr/models/uniasr/model.py:26                   |\n",
            "-----------    ** decoder_classes **    --------------\n",
            "| class name                                 | class location                                     |\n",
            "| ContextualParaformerDecoder                | funasr/models/contextual_paraformer/decoder.py:103 |\n",
            "| DynamicConvolution2DTransformerDecoder     | funasr/models/transformer/decoder.py:588           |\n",
            "| DynamicConvolutionTransformerDecoder       | funasr/models/transformer/decoder.py:527           |\n",
            "| FsmnDecoder                                | funasr/models/sanm/decoder.py:198                  |\n",
            "| FsmnDecoderSCAMAOpt                        | funasr/models/scama/decoder.py:197                 |\n",
            "| LightweightConvolution2DTransformerDecoder | funasr/models/transformer/decoder.py:465           |\n",
            "| LightweightConvolutionTransformerDecoder   | funasr/models/transformer/decoder.py:404           |\n",
            "| ParaformerSANDecoder                       | funasr/models/paraformer/decoder.py:529            |\n",
            "| ParaformerSANMDecoder                      | funasr/models/paraformer/decoder.py:204            |\n",
            "| TransformerDecoder                         | funasr/models/transformer/decoder.py:355           |\n",
            "-----------    ** normalize_classes **    --------------\n",
            "| class name   | class location                             |\n",
            "| GlobalMVN    | funasr/models/normalize/global_mvn.py:11   |\n",
            "| UtteranceMVN | funasr/models/normalize/utterance_mvn.py:8 |\n",
            "-----------    ** specaug_classes **    --------------\n",
            "| class name | class location                       |\n",
            "| SpecAug    | funasr/models/specaug/specaug.py:14  |\n",
            "| SpecAugLFR | funasr/models/specaug/specaug.py:104 |\n",
            "-----------    ** tokenizer_classes **    --------------\n",
            "| class name    | class location                        |\n",
            "| CharTokenizer | funasr/tokenizer/char_tokenizer.py:10 |\n",
            "\n",
            "\n",
            "2024-05-22 20:27:17,077 - modelscope - INFO - PyTorch version 2.3.0+cu121 Found.\n",
            "2024-05-22 20:27:17,079 - modelscope - INFO - TensorFlow version 2.15.0 Found.\n",
            "2024-05-22 20:27:17,079 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
            "2024-05-22 20:27:17,080 - modelscope - INFO - No valid ast index found from /root/.cache/modelscope/ast_indexer, generating ast index from prebuilt!\n",
            "2024-05-22 20:27:17,171 - modelscope - INFO - Loading done! Current index file version is 1.10.0, with md5 ded6a7273af1ea86481b8908505a159a and a total number of 946 components indexed\n",
            "2024-05-22 20:27:18,386 - modelscope - INFO - Use user-specified model revision: v2.0.4\n",
            "Downloading: 100% 10.9k/10.9k [00:00<00:00, 33.9MB/s]\n",
            "Downloading: 100% 173k/173k [00:00<00:00, 811kB/s]\n",
            "Downloading: 100% 2.45k/2.45k [00:00<00:00, 14.3MB/s]\n",
            "Downloading: 100% 472/472 [00:00<00:00, 2.48MB/s]\n",
            "Downloading: 100% 840M/840M [00:12<00:00, 71.0MB/s]\n",
            "Downloading: 100% 19.1k/19.1k [00:00<00:00, 388kB/s]\n",
            "Downloading: 100% 7.90M/7.90M [00:00<00:00, 15.2MB/s]\n",
            "Downloading: 100% 48.7k/48.7k [00:00<00:00, 466kB/s]\n",
            "Downloading: 100% 91.5k/91.5k [00:00<00:00, 624kB/s]\n",
            "2024-05-22 20:27:47,839 - modelscope - INFO - Use user-specified model revision: v2.0.4\n",
            "Downloading: 100% 7.85k/7.85k [00:00<00:00, 28.3MB/s]\n",
            "Downloading: 100% 1.19k/1.19k [00:00<00:00, 5.84MB/s]\n",
            "Downloading: 100% 365/365 [00:00<00:00, 2.17MB/s]\n",
            "Downloading: 100% 1.64M/1.64M [00:00<00:00, 3.83MB/s]\n",
            "Downloading: 100% 8.45k/8.45k [00:00<00:00, 31.3MB/s]\n",
            "Downloading: 100% 27.3k/27.3k [00:00<00:00, 530kB/s]\n",
            "Downloading: 100% 2.16M/2.16M [00:00<00:00, 5.21MB/s]\n",
            "2024-05-22 20:27:56,626 - modelscope - INFO - Use user-specified model revision: v2.0.4\n",
            "Downloading: 100% 6.00k/6.00k [00:00<00:00, 21.1MB/s]\n",
            "Downloading: 100% 810/810 [00:00<00:00, 4.43MB/s]\n",
            "Downloading: 100% 373/373 [00:00<00:00, 2.10MB/s]\n",
            "Downloading: 100% 278M/278M [00:04<00:00, 69.3MB/s]\n",
            "Downloading: 100% 863/863 [00:00<00:00, 4.90MB/s]\n",
            "Downloading: 100% 11.2k/11.2k [00:00<00:00, 34.5MB/s]\n",
            "Downloading: 100% 151k/151k [00:00<00:00, 773kB/s]\n",
            "Downloading: 100% 4.01M/4.01M [00:00<00:00, 9.15MB/s]\n",
            "  0% 0/5 [00:00<?, ?it/s]\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 1/2 [00:00<00:00,  2.05it/s]\u001b[A\n",
            "{'load_data': '0.126', 'extract_feat': '0.063', 'forward': '0.487', 'batch_size': '1', 'rtf': '0.130'}, :  50% 1/2 [00:00<00:00,  2.05it/s]\u001b[A\n",
            "rtf_avg: 0.130: 100% 2/2 [00:00<00:00,  4.10it/s]\n",
            "time cost vad: 0.488\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg_per_sample: 0.449, time_speech_total_per_sample:  3.760, time_escape_total_per_sample: 1.689:  67% 2/3 [00:01<00:00,  1.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.045', 'batch_size': '1', 'rtf': '-0.045'}, :  50% 1/2 [00:00<00:00, 22.38it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "rtf_avg: -0.045: 100% 2/2 [00:00<00:00, 44.14it/s]\n",
            "\n",
            " 50% 1/2 [00:01<00:01,  1.75s/it]\u001b[A\n",
            "rtf_avg_all_samples: 0.465, time_speech_total_all_samples:  3.760, time_escape_total_all_samples: 1.747: 100% 2/2 [00:01<00:00,  1.14it/s]\n",
            "rtf_avg_per_sample: 0.449, time_speech_total_per_sample:  3.760, time_escape_total_per_sample: 1.689:  67% 2/3 [00:01<00:00,  1.15it/s]\n",
            " 20% 1/5 [00:02<00:08,  2.24s/it]\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "{'load_data': '0.013', 'extract_feat': '0.007', 'forward': '0.058', 'batch_size': '1', 'rtf': '0.013'}, :  50% 1/2 [00:00<00:00, 17.16it/s]\u001b[A\n",
            "rtf_avg: 0.013: 100% 2/2 [00:00<00:00, 33.88it/s]\n",
            "time cost vad: 0.059\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg_per_sample: 0.021, time_speech_total_per_sample:  4.480, time_escape_total_per_sample: 0.096:  50% 1/2 [00:00<00:00, 10.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.013', 'batch_size': '1', 'rtf': '-0.013'}, :  50% 1/2 [00:00<00:00, 78.81it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "rtf_avg: -0.013: 100% 2/2 [00:00<00:00, 152.35it/s]\n",
            "\n",
            " 50% 1/2 [00:00<00:00,  8.22it/s]\u001b[A\n",
            "rtf_avg_all_samples: 0.027, time_speech_total_all_samples:  4.480, time_escape_total_all_samples: 0.122: 100% 2/2 [00:00<00:00, 16.40it/s]\n",
            "rtf_avg_per_sample: 0.021, time_speech_total_per_sample:  4.480, time_escape_total_per_sample: 0.096:  50% 1/2 [00:00<00:00,  9.13it/s]\n",
            " 40% 2/5 [00:02<00:03,  1.03s/it]\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "{'load_data': '0.018', 'extract_feat': '0.007', 'forward': '0.059', 'batch_size': '1', 'rtf': '0.014'}, :  50% 1/2 [00:00<00:00, 16.89it/s]\u001b[A\n",
            "rtf_avg: 0.014: 100% 2/2 [00:00<00:00, 33.37it/s]\n",
            "time cost vad: 0.060\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg_per_sample: 0.023, time_speech_total_per_sample:  4.170, time_escape_total_per_sample: 0.097:  50% 1/2 [00:00<00:00, 10.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.012', 'batch_size': '1', 'rtf': '-0.012'}, :  50% 1/2 [00:00<00:00, 81.53it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "rtf_avg: -0.012: 100% 2/2 [00:00<00:00, 157.03it/s]\n",
            "\n",
            " 50% 1/2 [00:00<00:00,  8.12it/s]\u001b[A\n",
            "rtf_avg_all_samples: 0.030, time_speech_total_all_samples:  4.170, time_escape_total_all_samples: 0.123: 100% 2/2 [00:00<00:00, 16.20it/s]\n",
            "rtf_avg_per_sample: 0.023, time_speech_total_per_sample:  4.170, time_escape_total_per_sample: 0.097:  50% 1/2 [00:00<00:00,  9.01it/s]\n",
            " 60% 3/5 [00:02<00:01,  1.56it/s]\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 1/2 [00:00<00:00,  7.14it/s]\u001b[A\n",
            "{'load_data': '0.022', 'extract_feat': '0.023', 'forward': '0.140', 'batch_size': '1', 'rtf': '0.011'}, :  50% 1/2 [00:00<00:00,  7.14it/s]\u001b[A\n",
            "rtf_avg: 0.011: 100% 2/2 [00:00<00:00, 14.16it/s]\n",
            "time cost vad: 0.142\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg_per_sample: 0.012, time_speech_total_per_sample:  12.860, time_escape_total_per_sample: 0.158:  75% 3/4 [00:00<00:00, 19.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.021', 'batch_size': '1', 'rtf': '-0.021'}, :  50% 1/2 [00:00<00:00, 47.70it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "rtf_avg: -0.021: 100% 2/2 [00:00<00:00, 93.94it/s]\n",
            "\n",
            " 50% 1/2 [00:00<00:00,  5.00it/s]\u001b[A\n",
            "rtf_avg_all_samples: 0.016, time_speech_total_all_samples:  12.860, time_escape_total_all_samples: 0.200: 100% 2/2 [00:00<00:00, 10.00it/s]\n",
            "rtf_avg_per_sample: 0.012, time_speech_total_per_sample:  12.860, time_escape_total_per_sample: 0.158:  75% 3/4 [00:00<00:00, 16.69it/s]\n",
            " 80% 4/5 [00:02<00:00,  1.91it/s]\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "{'load_data': '0.012', 'extract_feat': '0.008', 'forward': '0.048', 'batch_size': '1', 'rtf': '0.014'}, :  50% 1/2 [00:00<00:00, 20.93it/s]\u001b[A\n",
            "rtf_avg: 0.014: 100% 2/2 [00:00<00:00, 41.61it/s]\n",
            "time cost vad: 0.048\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg_per_sample: 0.026, time_speech_total_per_sample:  3.520, time_escape_total_per_sample: 0.090:  67% 2/3 [00:00<00:00, 22.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.006', 'batch_size': '1', 'rtf': '-0.006'}, :  50% 1/2 [00:00<00:00, 166.57it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "rtf_avg: -0.006: 100% 2/2 [00:00<00:00, 319.04it/s]\n",
            "\n",
            " 50% 1/2 [00:00<00:00,  9.22it/s]\u001b[A\n",
            "rtf_avg_all_samples: 0.031, time_speech_total_all_samples:  3.520, time_escape_total_all_samples: 0.109: 100% 2/2 [00:00<00:00, 18.40it/s]\n",
            "rtf_avg_per_sample: 0.026, time_speech_total_per_sample:  3.520, time_escape_total_per_sample: 0.090:  67% 2/3 [00:00<00:00, 20.52it/s]\n",
            "100% 5/5 [00:03<00:00,  1.61it/s]\n",
            "ASR 任务完成->标注文件路径: /content/GPT-SoVITS-v2/output/asr_opt/slicer_opt.list\n",
            "\n",
            "\"/usr/bin/python3\" tools/asr/funasr_asr.py -i \"output/slicer_opt\" -o \"output/asr_opt\" -s large -l zh -p float16\n",
            "Please install rotary_embedding_torch by: \n",
            " pip install -U rotary_embedding_torch\n",
            "Please install rotary_embedding_torch by: \n",
            " pip install -U rotary_embedding_torch\n",
            "Please install rotary_embedding_torch by: \n",
            " pip install -U rotary_embedding_torch\n",
            "Please install rotary_embedding_torch by: \n",
            " pip install -U rotary_embedding_torch\n",
            "\n",
            "tables: \n",
            "\n",
            "-----------    ** dataset_classes **    --------------\n",
            "| class name   | class location                               |\n",
            "| AudioDataset | funasr/datasets/audio_datasets/datasets.py:7 |\n",
            "-----------    ** index_ds_classes **    --------------\n",
            "| class name   | class location                               |\n",
            "| IndexDSJsonl | funasr/datasets/audio_datasets/index_ds.py:9 |\n",
            "-----------    ** batch_sampler_classes **    --------------\n",
            "| class name   | class location                               |\n",
            "| BatchSampler | funasr/datasets/audio_datasets/samplers.py:7 |\n",
            "-----------    ** frontend_classes **    --------------\n",
            "| class name        | class location                       |\n",
            "| WavFrontend       | funasr/frontends/wav_frontend.py:78  |\n",
            "| WavFrontendOnline | funasr/frontends/wav_frontend.py:216 |\n",
            "-----------    ** encoder_classes **    --------------\n",
            "| class name            | class location                                        |\n",
            "| BranchformerEncoder   | funasr/models/branchformer/encoder.py:294             |\n",
            "| ConformerChunkEncoder | funasr/models/bat/conformer_chunk_encoder.py:315      |\n",
            "| ConformerEncoder      | funasr/models/conformer/encoder.py:286                |\n",
            "| DFSMN                 | funasr/models/fsmn_vad_streaming/encoder.py:232       |\n",
            "| EBranchformerEncoder  | funasr/models/e_branchformer/encoder.py:177           |\n",
            "| FSMN                  | funasr/models/fsmn_vad_streaming/encoder.py:161       |\n",
            "| SANMEncoder           | funasr/models/sanm/encoder.py:161                     |\n",
            "| SANMEncoderChunkOpt   | funasr/models/scama/encoder.py:162                    |\n",
            "| SANMVadEncoder        | funasr/models/ct_transformer_streaming/encoder.py:148 |\n",
            "| TransformerEncoder    | funasr/models/transformer/encoder.py:139              |\n",
            "-----------    ** predictor_classes **    --------------\n",
            "| class name     | class location                                     |\n",
            "| CifPredictor   | funasr/models/paraformer/cif_predictor.py:15       |\n",
            "| CifPredictorV2 | funasr/models/paraformer/cif_predictor.py:141      |\n",
            "| CifPredictorV3 | funasr/models/bicif_paraformer/cif_predictor.py:95 |\n",
            "-----------    ** model_classes **    --------------\n",
            "| class name             | class location                                     |\n",
            "| BiCifParaformer        | funasr/models/bicif_paraformer/model.py:37         |\n",
            "| Branchformer           | funasr/models/branchformer/model.py:6              |\n",
            "| CAMPPlus               | funasr/models/campplus/model.py:30                 |\n",
            "| CTTransformer          | funasr/models/ct_transformer/model.py:30           |\n",
            "| CTTransformerStreaming | funasr/models/ct_transformer_streaming/model.py:27 |\n",
            "| Conformer              | funasr/models/conformer/model.py:8                 |\n",
            "| ContextualParaformer   | funasr/models/contextual_paraformer/model.py:43    |\n",
            "| EBranchformer          | funasr/models/e_branchformer/model.py:6            |\n",
            "| Emotion2vec            | funasr/models/emotion2vec/model.py:34              |\n",
            "| FsmnVADStreaming       | funasr/models/fsmn_vad_streaming/model.py:267      |\n",
            "| MonotonicAligner       | funasr/models/monotonic_aligner/model.py:24        |\n",
            "| Paraformer             | funasr/models/paraformer/model.py:26               |\n",
            "| ParaformerStreaming    | funasr/models/paraformer_streaming/model.py:37     |\n",
            "| SANM                   | funasr/models/sanm/model.py:13                     |\n",
            "| SCAMA                  | funasr/models/scama/model.py:38                    |\n",
            "| SeacoParaformer        | funasr/models/seaco_paraformer/model.py:45         |\n",
            "| Transformer            | funasr/models/transformer/model.py:20              |\n",
            "| UniASR                 | funasr/models/uniasr/model.py:26                   |\n",
            "-----------    ** decoder_classes **    --------------\n",
            "| class name                                 | class location                                     |\n",
            "| ContextualParaformerDecoder                | funasr/models/contextual_paraformer/decoder.py:103 |\n",
            "| DynamicConvolution2DTransformerDecoder     | funasr/models/transformer/decoder.py:588           |\n",
            "| DynamicConvolutionTransformerDecoder       | funasr/models/transformer/decoder.py:527           |\n",
            "| FsmnDecoder                                | funasr/models/sanm/decoder.py:198                  |\n",
            "| FsmnDecoderSCAMAOpt                        | funasr/models/scama/decoder.py:197                 |\n",
            "| LightweightConvolution2DTransformerDecoder | funasr/models/transformer/decoder.py:465           |\n",
            "| LightweightConvolutionTransformerDecoder   | funasr/models/transformer/decoder.py:404           |\n",
            "| ParaformerSANDecoder                       | funasr/models/paraformer/decoder.py:529            |\n",
            "| ParaformerSANMDecoder                      | funasr/models/paraformer/decoder.py:204            |\n",
            "| TransformerDecoder                         | funasr/models/transformer/decoder.py:355           |\n",
            "-----------    ** normalize_classes **    --------------\n",
            "| class name   | class location                             |\n",
            "| GlobalMVN    | funasr/models/normalize/global_mvn.py:11   |\n",
            "| UtteranceMVN | funasr/models/normalize/utterance_mvn.py:8 |\n",
            "-----------    ** specaug_classes **    --------------\n",
            "| class name | class location                       |\n",
            "| SpecAug    | funasr/models/specaug/specaug.py:14  |\n",
            "| SpecAugLFR | funasr/models/specaug/specaug.py:104 |\n",
            "-----------    ** tokenizer_classes **    --------------\n",
            "| class name    | class location                        |\n",
            "| CharTokenizer | funasr/tokenizer/char_tokenizer.py:10 |\n",
            "\n",
            "\n",
            "2024-05-22 20:29:19,090 - modelscope - INFO - PyTorch version 2.3.0+cu121 Found.\n",
            "2024-05-22 20:29:19,091 - modelscope - INFO - TensorFlow version 2.15.0 Found.\n",
            "2024-05-22 20:29:19,091 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
            "2024-05-22 20:29:19,129 - modelscope - INFO - Loading done! Current index file version is 1.10.0, with md5 ded6a7273af1ea86481b8908505a159a and a total number of 946 components indexed\n",
            "2024-05-22 20:29:20,092 - modelscope - INFO - Use user-specified model revision: v2.0.4\n",
            "2024-05-22 20:29:26,233 - modelscope - INFO - Use user-specified model revision: v2.0.4\n",
            "2024-05-22 20:29:27,593 - modelscope - INFO - Use user-specified model revision: v2.0.4\n",
            "  0% 0/5 [00:00<?, ?it/s]\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 1/2 [00:00<00:00,  7.90it/s]\u001b[A\n",
            "{'load_data': '0.017', 'extract_feat': '0.008', 'forward': '0.127', 'batch_size': '1', 'rtf': '0.034'}, :  50% 1/2 [00:00<00:00,  7.90it/s]\u001b[A\n",
            "rtf_avg: 0.034: 100% 2/2 [00:00<00:00, 15.68it/s]\n",
            "time cost vad: 0.128\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg_per_sample: 0.233, time_speech_total_per_sample:  3.760, time_escape_total_per_sample: 0.877:  67% 2/3 [00:00<00:00,  2.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.024', 'batch_size': '1', 'rtf': '-0.024'}, :  50% 1/2 [00:00<00:00, 42.32it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "rtf_avg: -0.024: 100% 2/2 [00:00<00:00, 83.00it/s]\n",
            "\n",
            " 50% 1/2 [00:00<00:00,  1.08it/s]\u001b[A\n",
            "rtf_avg_all_samples: 0.245, time_speech_total_all_samples:  3.760, time_escape_total_all_samples: 0.922: 100% 2/2 [00:00<00:00,  2.17it/s]\n",
            "rtf_avg_per_sample: 0.233, time_speech_total_per_sample:  3.760, time_escape_total_per_sample: 0.877:  67% 2/3 [00:00<00:00,  2.22it/s]\n",
            " 20% 1/5 [00:01<00:04,  1.05s/it]\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "{'load_data': '0.015', 'extract_feat': '0.008', 'forward': '0.059', 'batch_size': '1', 'rtf': '0.013'}, :  50% 1/2 [00:00<00:00, 17.04it/s]\u001b[A\n",
            "rtf_avg: 0.013: 100% 2/2 [00:00<00:00, 33.71it/s]\n",
            "time cost vad: 0.060\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg_per_sample: 0.023, time_speech_total_per_sample:  4.480, time_escape_total_per_sample: 0.105:  50% 1/2 [00:00<00:00,  9.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.013', 'batch_size': '1', 'rtf': '-0.013'}, :  50% 1/2 [00:00<00:00, 74.03it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "rtf_avg: -0.013: 100% 2/2 [00:00<00:00, 142.94it/s]\n",
            "\n",
            " 50% 1/2 [00:00<00:00,  7.59it/s]\u001b[A\n",
            "rtf_avg_all_samples: 0.029, time_speech_total_all_samples:  4.480, time_escape_total_all_samples: 0.132: 100% 2/2 [00:00<00:00, 15.15it/s]\n",
            "rtf_avg_per_sample: 0.023, time_speech_total_per_sample:  4.480, time_escape_total_per_sample: 0.105:  50% 1/2 [00:00<00:00,  8.38it/s]\n",
            " 40% 2/5 [00:01<00:01,  1.83it/s]\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "{'load_data': '0.020', 'extract_feat': '0.007', 'forward': '0.061', 'batch_size': '1', 'rtf': '0.015'}, :  50% 1/2 [00:00<00:00, 16.27it/s]\u001b[A\n",
            "rtf_avg: 0.015: 100% 2/2 [00:00<00:00, 32.20it/s]\n",
            "time cost vad: 0.062\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg_per_sample: 0.022, time_speech_total_per_sample:  4.170, time_escape_total_per_sample: 0.091:  50% 1/2 [00:00<00:00, 10.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.012', 'batch_size': '1', 'rtf': '-0.012'}, :  50% 1/2 [00:00<00:00, 81.56it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "rtf_avg: -0.012: 100% 2/2 [00:00<00:00, 157.75it/s]\n",
            "\n",
            " 50% 1/2 [00:00<00:00,  8.60it/s]\u001b[A\n",
            "rtf_avg_all_samples: 0.028, time_speech_total_all_samples:  4.170, time_escape_total_all_samples: 0.117: 100% 2/2 [00:00<00:00, 17.16it/s]\n",
            "rtf_avg_per_sample: 0.022, time_speech_total_per_sample:  4.170, time_escape_total_per_sample: 0.091:  50% 1/2 [00:00<00:00,  9.56it/s]\n",
            " 60% 3/5 [00:01<00:00,  2.64it/s]\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 1/2 [00:00<00:00,  7.48it/s]\u001b[A\n",
            "{'load_data': '0.021', 'extract_feat': '0.022', 'forward': '0.134', 'batch_size': '1', 'rtf': '0.010'}, :  50% 1/2 [00:00<00:00,  7.48it/s]\u001b[A\n",
            "rtf_avg: 0.010: 100% 2/2 [00:00<00:00, 14.84it/s]\n",
            "time cost vad: 0.135\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg_per_sample: 0.011, time_speech_total_per_sample:  12.860, time_escape_total_per_sample: 0.141:  75% 3/4 [00:00<00:00, 21.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.020', 'batch_size': '1', 'rtf': '-0.020'}, :  50% 1/2 [00:00<00:00, 50.60it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "rtf_avg: -0.020: 100% 2/2 [00:00<00:00, 99.87it/s] \n",
            "\n",
            " 50% 1/2 [00:00<00:00,  5.53it/s]\u001b[A\n",
            "rtf_avg_all_samples: 0.014, time_speech_total_all_samples:  12.860, time_escape_total_all_samples: 0.181: 100% 2/2 [00:00<00:00, 11.05it/s]\n",
            "rtf_avg_per_sample: 0.011, time_speech_total_per_sample:  12.860, time_escape_total_per_sample: 0.141:  75% 3/4 [00:00<00:00, 18.56it/s]\n",
            " 80% 4/5 [00:01<00:00,  2.82it/s]\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "{'load_data': '0.012', 'extract_feat': '0.005', 'forward': '0.044', 'batch_size': '1', 'rtf': '0.012'}, :  50% 1/2 [00:00<00:00, 22.86it/s]\u001b[A\n",
            "rtf_avg: 0.012: 100% 2/2 [00:00<00:00, 45.45it/s]\n",
            "time cost vad: 0.044\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg_per_sample: 0.025, time_speech_total_per_sample:  3.520, time_escape_total_per_sample: 0.090:  67% 2/3 [00:00<00:00, 22.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.006', 'batch_size': '1', 'rtf': '-0.006'}, :  50% 1/2 [00:00<00:00, 173.18it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "rtf_avg: -0.006: 100% 2/2 [00:00<00:00, 331.41it/s]\n",
            "\n",
            " 50% 1/2 [00:00<00:00,  9.21it/s]\u001b[A\n",
            "rtf_avg_all_samples: 0.031, time_speech_total_all_samples:  3.520, time_escape_total_all_samples: 0.109: 100% 2/2 [00:00<00:00, 18.38it/s]\n",
            "rtf_avg_per_sample: 0.025, time_speech_total_per_sample:  3.520, time_escape_total_per_sample: 0.090:  67% 2/3 [00:00<00:00, 20.79it/s]\n",
            "100% 5/5 [00:01<00:00,  2.64it/s]\n",
            "ASR 任务完成->标注文件路径: /content/GPT-SoVITS-v2/output/asr_opt/slicer_opt.list\n",
            "\n",
            "\"/usr/bin/python3\" GPT_SoVITS/prepare_datasets/1-get-text.py\n",
            "\"/usr/bin/python3\" GPT_SoVITS/prepare_datasets/1-get-text.py\n",
            "Building prefix dict from the default dictionary ...\n",
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /content/GPT-SoVITS-v2/TEMP/jieba.cache\n",
            "Dumping model to file cache /content/GPT-SoVITS-v2/TEMP/jieba.cache\n",
            "Loading model cost 1.271 seconds.\n",
            "Prefix dict has been built succesfully.\n",
            "Loading model cost 1.297 seconds.\n",
            "Prefix dict has been built succesfully.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/GPT-SoVITS-v2/webui.py\", line 586, in open1abc\n",
            "    yield \"进度：1a-done\", {\"__type__\": \"update\", \"visible\": False}, {\"__type__\": \"update\", \"visible\": True}\n",
            "GeneratorExit\n",
            "Exception ignored in: <generator object open1abc at 0x78235c583920>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 691, in gen_wrapper\n",
            "    yield from f(*args, **kwargs)\n",
            "RuntimeError: generator ignored GeneratorExit\n",
            "\"/usr/bin/python3\" GPT_SoVITS/prepare_datasets/2-get-hubert-wav32k.py\n",
            "\"/usr/bin/python3\" GPT_SoVITS/prepare_datasets/2-get-hubert-wav32k.py\n",
            "Some weights of the model checkpoint at GPT_SoVITS/pretrained_models/chinese-hubert-base were not used when initializing HubertModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at GPT_SoVITS/pretrained_models/chinese-hubert-base were not used when initializing HubertModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of HubertModel were not initialized from the model checkpoint at GPT_SoVITS/pretrained_models/chinese-hubert-base and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of HubertModel were not initialized from the model checkpoint at GPT_SoVITS/pretrained_models/chinese-hubert-base and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\"/usr/bin/python3\" GPT_SoVITS/prepare_datasets/3-get-semantic.py\n",
            "\"/usr/bin/python3\" GPT_SoVITS/prepare_datasets/3-get-semantic.py\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "<All keys matched successfully>\n",
            "<All keys matched successfully>\n",
            "\"/usr/bin/python3\" GPT_SoVITS/s2_train.py --config \"/content/GPT-SoVITS-v2/TEMP/tmp_s2.json\"\n",
            "2024-05-22 20:32:43.696111: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:32:43.696161: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:32:43.803019: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-22 20:32:44.008091: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "2024-05-22 20:32:46.037953: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "2024-05-22 20:32:51.392889: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:32:51.392941: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:32:51.394152: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "2024-05-22 20:32:52.578716: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:CXX:{'train': {'log_interval': 100, 'eval_interval': 500, 'seed': 1234, 'epochs': 8, 'learning_rate': 0.0001, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 7, 'fp16_run': True, 'lr_decay': 0.999875, 'segment_size': 20480, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'text_low_lr_rate': 0.4, 'pretrained_s2G': 'GPT_SoVITS/pretrained_models/s2G488k.pth', 'pretrained_s2D': 'GPT_SoVITS/pretrained_models/s2D488k.pth', 'if_save_latest': True, 'if_save_every_weights': True, 'save_every_epoch': 4, 'gpu_numbers': '0'}, 'data': {'max_wav_value': 32768.0, 'sampling_rate': 32000, 'filter_length': 2048, 'hop_length': 640, 'win_length': 2048, 'n_mel_channels': 128, 'mel_fmin': 0.0, 'mel_fmax': None, 'add_blank': True, 'n_speakers': 300, 'cleaned_text': True, 'exp_dir': 'logs/CXX'}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [10, 8, 2, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 8, 2, 2], 'n_layers_q': 3, 'use_spectral_norm': False, 'gin_channels': 512, 'semantic_frame_rate': '25hz', 'freeze_quantizer': True}, 's2_ckpt_dir': 'logs/CXX', 'content_module': 'cnhubert', 'save_weight_dir': 'SoVITS_weights', 'name': 'CXX', 'pretrain': None, 'resume_step': None}\n",
            "phoneme_data_len: 5\n",
            "wav_data_len: 100\n",
            "100% 100/100 [00:00<00:00, 47738.49it/s]\n",
            "skipped_phone:  0 , skipped_dur:  0\n",
            "total left:  100\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "ssl_proj.weight not requires_grad\n",
            "ssl_proj.bias not requires_grad\n",
            "INFO:CXX:loaded pretrained GPT_SoVITS/pretrained_models/s2G488k.pth\n",
            "<All keys matched successfully>\n",
            "INFO:CXX:loaded pretrained GPT_SoVITS/pretrained_models/s2D488k.pth\n",
            "<All keys matched successfully>\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "0it [00:00, ?it/s]2024-05-22 20:33:15.639526: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:33:15.639576: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:33:15.641249: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-22 20:33:15.647009: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:33:15.647049: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:33:15.648597: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-22 20:33:15.735360: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:33:15.735399: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:33:15.737013: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-22 20:33:15.979497: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:33:15.979549: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:33:15.979497: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:33:15.987729: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:33:15.991283: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-22 20:33:15.994616: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:33:15.994791: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-22 20:33:16.002862: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:33:16.005332: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "2024-05-22 20:33:23.563209: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-05-22 20:33:23.602192: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-05-22 20:33:23.641791: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-05-22 20:33:23.883390: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-05-22 20:33:24.001466: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "2024-05-22 20:33:24.089463: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "[rank0]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: ComplexHalf support is experimental and many operators don't support it yet. (Triggered internally at ../aten/src/ATen/EmptyTensor.cpp:41.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv1d(input, weight, bias, self.stride,\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
            "grad.sizes() = [1, 9, 96], strides() = [43296, 96, 1]\n",
            "bucket_view.sizes() = [1, 9, 96], strides() = [864, 96, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:325.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "INFO:CXX:Train Epoch: 1 [0%]\n",
            "INFO:CXX:[2.228034257888794, 2.3240177631378174, 11.463974952697754, 22.901634216308594, 0.560517430305481, 2.4982030391693115, 0, 9.99875e-05]\n",
            "15it [01:10,  4.71s/it]\n",
            "INFO:CXX:====> Epoch: 1\n",
            "15it [00:24,  1.60s/it]\n",
            "INFO:CXX:====> Epoch: 2\n",
            "15it [00:23,  1.54s/it]\n",
            "INFO:CXX:====> Epoch: 3\n",
            "15it [00:23,  1.58s/it]\n",
            "INFO:CXX:Saving model and optimizer state at iteration 4 to logs/CXX/logs_s2/G_233333333333.pth\n",
            "INFO:CXX:Saving model and optimizer state at iteration 4 to logs/CXX/logs_s2/D_233333333333.pth\n",
            "INFO:CXX:saving ckpt CXX_e4:Success.\n",
            "INFO:CXX:====> Epoch: 4\n",
            "15it [00:22,  1.53s/it]\n",
            "INFO:CXX:====> Epoch: 5\n",
            "15it [00:23,  1.58s/it]\n",
            "INFO:CXX:====> Epoch: 6\n",
            "10it [00:21,  1.21it/s]INFO:CXX:Train Epoch: 7 [67%]\n",
            "INFO:CXX:[2.268226146697998, 3.2003114223480225, 13.043952941894531, 22.32061195373535, 0.40251004695892334, 1.4287325143814087, 100, 9.991253280566489e-05]\n",
            "15it [00:24,  1.66s/it]\n",
            "INFO:CXX:====> Epoch: 7\n",
            "15it [00:23,  1.59s/it]\n",
            "INFO:CXX:Saving model and optimizer state at iteration 8 to logs/CXX/logs_s2/G_233333333333.pth\n",
            "INFO:CXX:Saving model and optimizer state at iteration 8 to logs/CXX/logs_s2/D_233333333333.pth\n",
            "INFO:CXX:saving ckpt CXX_e8:Success.\n",
            "INFO:CXX:====> Epoch: 8\n",
            "\"/usr/bin/python3\" GPT_SoVITS/s2_train.py --config \"/content/GPT-SoVITS-v2/TEMP/tmp_s2.json\"\n",
            "2024-05-22 20:38:54.748775: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:38:54.748831: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:38:54.750299: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-22 20:38:54.757910: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "2024-05-22 20:38:56.090175: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "2024-05-22 20:39:01.755476: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:39:01.755532: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:39:01.757252: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "2024-05-22 20:39:03.454303: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:CXX:{'train': {'log_interval': 100, 'eval_interval': 500, 'seed': 1234, 'epochs': 8, 'learning_rate': 0.0001, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 7, 'fp16_run': True, 'lr_decay': 0.999875, 'segment_size': 20480, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'text_low_lr_rate': 0.4, 'pretrained_s2G': 'GPT_SoVITS/pretrained_models/s2G488k.pth', 'pretrained_s2D': 'GPT_SoVITS/pretrained_models/s2D488k.pth', 'if_save_latest': True, 'if_save_every_weights': True, 'save_every_epoch': 4, 'gpu_numbers': '0'}, 'data': {'max_wav_value': 32768.0, 'sampling_rate': 32000, 'filter_length': 2048, 'hop_length': 640, 'win_length': 2048, 'n_mel_channels': 128, 'mel_fmin': 0.0, 'mel_fmax': None, 'add_blank': True, 'n_speakers': 300, 'cleaned_text': True, 'exp_dir': 'logs/CXX'}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [10, 8, 2, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 8, 2, 2], 'n_layers_q': 3, 'use_spectral_norm': False, 'gin_channels': 512, 'semantic_frame_rate': '25hz', 'freeze_quantizer': True}, 's2_ckpt_dir': 'logs/CXX', 'content_module': 'cnhubert', 'save_weight_dir': 'SoVITS_weights', 'name': 'CXX', 'pretrain': None, 'resume_step': None}\n",
            "phoneme_data_len: 5\n",
            "wav_data_len: 100\n",
            "100% 100/100 [00:00<00:00, 80566.73it/s]\n",
            "skipped_phone:  0 , skipped_dur:  0\n",
            "total left:  100\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "ssl_proj.weight not requires_grad\n",
            "ssl_proj.bias not requires_grad\n",
            "logs/CXX/logs_s2/D_233333333333.pth\n",
            "load \n",
            "INFO:CXX:Loaded checkpoint 'logs/CXX/logs_s2/D_233333333333.pth' (iteration 8)\n",
            "INFO:CXX:loaded D\n",
            "logs/CXX/logs_s2/G_233333333333.pth\n",
            "load \n",
            "INFO:CXX:Loaded checkpoint 'logs/CXX/logs_s2/G_233333333333.pth' (iteration 8)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "0it [00:00, ?it/s]/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 27 leaked semaphore objects to clean up at shutdown\n",
            "  warnings.warn('resource_tracker: There appear to be %d '\n",
            "\"/usr/bin/python3\" GPT_SoVITS/s2_train.py --config \"/content/GPT-SoVITS-v2/TEMP/tmp_s2.json\"\n",
            "2024-05-22 20:39:37.027906: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:39:37.027969: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:39:37.029898: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-22 20:39:37.041343: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "2024-05-22 20:39:38.749008: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "2024-05-22 20:39:43.886673: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:39:43.886723: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:39:43.887962: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "2024-05-22 20:39:44.996144: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:CXX:{'train': {'log_interval': 100, 'eval_interval': 500, 'seed': 1234, 'epochs': 8, 'learning_rate': 0.0001, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 7, 'fp16_run': True, 'lr_decay': 0.999875, 'segment_size': 20480, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'text_low_lr_rate': 0.4, 'pretrained_s2G': 'GPT_SoVITS/pretrained_models/s2G488k.pth', 'pretrained_s2D': 'GPT_SoVITS/pretrained_models/s2D488k.pth', 'if_save_latest': True, 'if_save_every_weights': True, 'save_every_epoch': 4, 'gpu_numbers': '0'}, 'data': {'max_wav_value': 32768.0, 'sampling_rate': 32000, 'filter_length': 2048, 'hop_length': 640, 'win_length': 2048, 'n_mel_channels': 128, 'mel_fmin': 0.0, 'mel_fmax': None, 'add_blank': True, 'n_speakers': 300, 'cleaned_text': True, 'exp_dir': 'logs/CXX'}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [10, 8, 2, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 8, 2, 2], 'n_layers_q': 3, 'use_spectral_norm': False, 'gin_channels': 512, 'semantic_frame_rate': '25hz', 'freeze_quantizer': True}, 's2_ckpt_dir': 'logs/CXX', 'content_module': 'cnhubert', 'save_weight_dir': 'SoVITS_weights', 'name': 'CXX', 'pretrain': None, 'resume_step': None}\n",
            "phoneme_data_len: 5\n",
            "wav_data_len: 100\n",
            "100% 100/100 [00:00<00:00, 65731.14it/s]\n",
            "skipped_phone:  0 , skipped_dur:  0\n",
            "total left:  100\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "ssl_proj.weight not requires_grad\n",
            "ssl_proj.bias not requires_grad\n",
            "logs/CXX/logs_s2/D_233333333333.pth\n",
            "load \n",
            "INFO:CXX:Loaded checkpoint 'logs/CXX/logs_s2/D_233333333333.pth' (iteration 8)\n",
            "INFO:CXX:loaded D\n",
            "logs/CXX/logs_s2/G_233333333333.pth\n",
            "load \n",
            "INFO:CXX:Loaded checkpoint 'logs/CXX/logs_s2/G_233333333333.pth' (iteration 8)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "0it [00:00, ?it/s]2024-05-22 20:40:08.273594: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:40:08.283528: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:40:08.289642: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-22 20:40:08.412023: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:40:08.416847: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:40:08.418519: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-22 20:40:08.457296: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:40:08.459846: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:40:08.461538: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-22 20:40:08.483096: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:40:08.487849: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:40:08.489496: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-22 20:40:08.644995: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:40:08.652874: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:40:08.656530: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-22 20:40:08.996076: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:40:09.005862: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:40:09.007613: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "2024-05-22 20:40:15.111124: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-05-22 20:40:15.234386: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "2024-05-22 20:40:15.484224: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-05-22 20:40:15.554493: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-05-22 20:40:15.573533: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "2024-05-22 20:40:16.540493: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "[rank0]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: ComplexHalf support is experimental and many operators don't support it yet. (Triggered internally at ../aten/src/ATen/EmptyTensor.cpp:41.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv1d(input, weight, bias, self.stride,\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
            "grad.sizes() = [1, 9, 96], strides() = [43296, 96, 1]\n",
            "bucket_view.sizes() = [1, 9, 96], strides() = [864, 96, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:325.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "15it [01:13,  4.88s/it]\n",
            "INFO:CXX:Saving model and optimizer state at iteration 8 to logs/CXX/logs_s2/G_233333333333.pth\n",
            "INFO:CXX:Saving model and optimizer state at iteration 8 to logs/CXX/logs_s2/D_233333333333.pth\n",
            "INFO:CXX:saving ckpt CXX_e8:Success.\n",
            "INFO:CXX:====> Epoch: 8\n",
            "\"/usr/bin/python3\" GPT_SoVITS/s2_train.py --config \"/content/GPT-SoVITS-v2/TEMP/tmp_s2.json\"\n",
            "2024-05-22 20:43:59.826398: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:43:59.826473: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:43:59.828780: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-22 20:43:59.840842: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "2024-05-22 20:44:01.671003: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "2024-05-22 20:44:06.888223: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:44:06.888283: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:44:06.889700: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "2024-05-22 20:44:08.134071: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:CXX:{'train': {'log_interval': 100, 'eval_interval': 500, 'seed': 1234, 'epochs': 8, 'learning_rate': 0.0001, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 7, 'fp16_run': True, 'lr_decay': 0.999875, 'segment_size': 20480, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'text_low_lr_rate': 0.4, 'pretrained_s2G': 'GPT_SoVITS/pretrained_models/s2G488k.pth', 'pretrained_s2D': 'GPT_SoVITS/pretrained_models/s2D488k.pth', 'if_save_latest': True, 'if_save_every_weights': True, 'save_every_epoch': 4, 'gpu_numbers': '0'}, 'data': {'max_wav_value': 32768.0, 'sampling_rate': 32000, 'filter_length': 2048, 'hop_length': 640, 'win_length': 2048, 'n_mel_channels': 128, 'mel_fmin': 0.0, 'mel_fmax': None, 'add_blank': True, 'n_speakers': 300, 'cleaned_text': True, 'exp_dir': 'logs/CXX'}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [10, 8, 2, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 8, 2, 2], 'n_layers_q': 3, 'use_spectral_norm': False, 'gin_channels': 512, 'semantic_frame_rate': '25hz', 'freeze_quantizer': True}, 's2_ckpt_dir': 'logs/CXX', 'content_module': 'cnhubert', 'save_weight_dir': 'SoVITS_weights', 'name': 'CXX', 'pretrain': None, 'resume_step': None}\n",
            "phoneme_data_len: 5\n",
            "wav_data_len: 100\n",
            "100% 100/100 [00:00<00:00, 77801.97it/s]\n",
            "skipped_phone:  0 , skipped_dur:  0\n",
            "total left:  100\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "ssl_proj.weight not requires_grad\n",
            "ssl_proj.bias not requires_grad\n",
            "logs/CXX/logs_s2/D_233333333333.pth\n",
            "load \n",
            "INFO:CXX:Loaded checkpoint 'logs/CXX/logs_s2/D_233333333333.pth' (iteration 8)\n",
            "INFO:CXX:loaded D\n",
            "logs/CXX/logs_s2/G_233333333333.pth\n",
            "load \n",
            "INFO:CXX:Loaded checkpoint 'logs/CXX/logs_s2/G_233333333333.pth' (iteration 8)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "0it [00:00, ?it/s]2024-05-22 20:44:35.529156: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:44:35.529213: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:44:35.538184: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-22 20:44:35.914318: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:44:35.914376: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:44:35.916334: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-22 20:44:35.931852: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:44:35.931901: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:44:35.941068: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-22 20:44:36.098299: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:44:36.107843: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:44:36.112746: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-22 20:44:36.126307: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:44:36.126353: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:44:36.128019: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-22 20:44:36.672997: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:44:36.684845: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:44:36.686811: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "2024-05-22 20:44:43.406347: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "2024-05-22 20:44:43.699319: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-05-22 20:44:43.747981: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-05-22 20:44:43.788816: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "2024-05-22 20:44:43.917326: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-05-22 20:44:44.106573: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "0it [00:40, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 27 leaked semaphore objects to clean up at shutdown\n",
            "  warnings.warn('resource_tracker: There appear to be %d '\n",
            "\"/usr/bin/python3\" GPT_SoVITS/s2_train.py --config \"/content/GPT-SoVITS-v2/TEMP/tmp_s2.json\"\n",
            "2024-05-22 20:45:05.987024: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:45:05.987088: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:45:05.988605: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-22 20:45:05.996747: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "2024-05-22 20:45:07.227301: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "2024-05-22 20:45:13.048229: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:45:13.048295: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:45:13.050189: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "2024-05-22 20:45:14.870083: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:CXX:{'train': {'log_interval': 100, 'eval_interval': 500, 'seed': 1234, 'epochs': 8, 'learning_rate': 0.0001, 'betas': [0.8, 0.99], 'eps': 1e-09, 'batch_size': 7, 'fp16_run': True, 'lr_decay': 0.999875, 'segment_size': 20480, 'init_lr_ratio': 1, 'warmup_epochs': 0, 'c_mel': 45, 'c_kl': 1.0, 'text_low_lr_rate': 0.4, 'pretrained_s2G': 'GPT_SoVITS/pretrained_models/s2G488k.pth', 'pretrained_s2D': 'GPT_SoVITS/pretrained_models/s2D488k.pth', 'if_save_latest': True, 'if_save_every_weights': True, 'save_every_epoch': 4, 'gpu_numbers': '0'}, 'data': {'max_wav_value': 32768.0, 'sampling_rate': 32000, 'filter_length': 2048, 'hop_length': 640, 'win_length': 2048, 'n_mel_channels': 128, 'mel_fmin': 0.0, 'mel_fmax': None, 'add_blank': True, 'n_speakers': 300, 'cleaned_text': True, 'exp_dir': 'logs/CXX'}, 'model': {'inter_channels': 192, 'hidden_channels': 192, 'filter_channels': 768, 'n_heads': 2, 'n_layers': 6, 'kernel_size': 3, 'p_dropout': 0.1, 'resblock': '1', 'resblock_kernel_sizes': [3, 7, 11], 'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]], 'upsample_rates': [10, 8, 2, 2, 2], 'upsample_initial_channel': 512, 'upsample_kernel_sizes': [16, 16, 8, 2, 2], 'n_layers_q': 3, 'use_spectral_norm': False, 'gin_channels': 512, 'semantic_frame_rate': '25hz', 'freeze_quantizer': True}, 's2_ckpt_dir': 'logs/CXX', 'content_module': 'cnhubert', 'save_weight_dir': 'SoVITS_weights', 'name': 'CXX', 'pretrain': None, 'resume_step': None}\n",
            "phoneme_data_len: 5\n",
            "wav_data_len: 100\n",
            "100% 100/100 [00:00<00:00, 73843.38it/s]\n",
            "skipped_phone:  0 , skipped_dur:  0\n",
            "total left:  100\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "ssl_proj.weight not requires_grad\n",
            "ssl_proj.bias not requires_grad\n",
            "logs/CXX/logs_s2/D_233333333333.pth\n",
            "load \n",
            "INFO:CXX:Loaded checkpoint 'logs/CXX/logs_s2/D_233333333333.pth' (iteration 8)\n",
            "INFO:CXX:loaded D\n",
            "logs/CXX/logs_s2/G_233333333333.pth\n",
            "load \n",
            "INFO:CXX:Loaded checkpoint 'logs/CXX/logs_s2/G_233333333333.pth' (iteration 8)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "0it [00:00, ?it/s]2024-05-22 20:45:41.495859: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:45:41.495959: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:45:41.504492: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:45:41.505238: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-22 20:45:41.509842: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:45:41.511616: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-22 20:45:41.590298: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:45:41.591800: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:45:41.591858: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:45:41.593662: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-22 20:45:41.594362: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:45:41.594405: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:45:41.600844: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:45:41.602477: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-22 20:45:41.602698: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-22 20:45:42.107084: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:45:42.112856: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:45:42.120694: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "2024-05-22 20:45:47.093520: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "2024-05-22 20:45:47.492648: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-05-22 20:45:47.512150: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-05-22 20:45:47.539700: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-05-22 20:45:47.883245: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "2024-05-22 20:45:48.079900: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "[rank0]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: ComplexHalf support is experimental and many operators don't support it yet. (Triggered internally at ../aten/src/ATen/EmptyTensor.cpp:41.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv1d(input, weight, bias, self.stride,\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
            "grad.sizes() = [1, 9, 96], strides() = [43296, 96, 1]\n",
            "bucket_view.sizes() = [1, 9, 96], strides() = [864, 96, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:325.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "15it [01:12,  4.86s/it]\n",
            "INFO:CXX:Saving model and optimizer state at iteration 8 to logs/CXX/logs_s2/G_233333333333.pth\n",
            "INFO:CXX:Saving model and optimizer state at iteration 8 to logs/CXX/logs_s2/D_233333333333.pth\n",
            "INFO:CXX:saving ckpt CXX_e8:Success.\n",
            "INFO:CXX:====> Epoch: 8\n",
            "\"/usr/bin/python3\" GPT_SoVITS/s1_train.py --config_file \"/content/GPT-SoVITS-v2/TEMP/tmp_s1.yaml\" \n",
            "Seed set to 1234\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "<All keys matched successfully>\n",
            "ckpt_path: None\n",
            "[rank: 0] Seed set to 1234\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Missing logger folder: logs/CXX/logs_s1/logs_s1\n",
            "2024-05-22 20:47:41.888775: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-22 20:47:41.888849: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-22 20:47:41.891726: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-22 20:47:41.902272: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-05-22 20:47:43.807213: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "semantic_data_len: 5\n",
            "phoneme_data_len: 5\n",
            "                           item_name                                     semantic_audio\n",
            "0  CXY.MP3_0000001280_0000121600.wav  208 656 185 334 656 656 103 656 334 656 451 31...\n",
            "1  CXY.MP3_0000276160_0000409600.wav  208 656 656 581 930 581 273 237 273 699 570 72...\n",
            "2  CXY.MP3_0000821120_0000933760.wav  208 656 656 758 526 754 433 420 185 746 185 42...\n",
            "3  CXY.MP3_0000132800_0000276160.wav  208 451 208 451 451 451 160 318 214 436 36 36 ...\n",
            "4  CXY.MP3_0000409600_0000821120.wav  596 28 633 637 346 592 914 61 231 451 14 722 6...\n",
            "dataset.__len__(): 100\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                 | Params\n",
            "-----------------------------------------------\n",
            "0 | model | Text2SemanticDecoder | 77.5 M\n",
            "-----------------------------------------------\n",
            "77.5 M    Trainable params\n",
            "0         Non-trainable params\n",
            "77.5 M    Total params\n",
            "309.975   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (15) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "Epoch 14: 100% 15/15 [00:03<00:00,  4.70it/s, v_num=0, total_loss_step=16.80, lr_step=0.002, top_3_acc_step=1.000, total_loss_epoch=126.0, lr_epoch=0.002, top_3_acc_epoch=1.000]`Trainer.fit` stopped: `max_epochs=15` reached.\n",
            "Epoch 14: 100% 15/15 [00:10<00:00,  1.43it/s, v_num=0, total_loss_step=16.80, lr_step=0.002, top_3_acc_step=1.000, total_loss_epoch=126.0, lr_epoch=0.002, top_3_acc_epoch=1.000]\n",
            "\"/usr/bin/python3\" GPT_SoVITS/inference_webui.py\n",
            "Some weights of the model checkpoint at GPT_SoVITS/pretrained_models/chinese-hubert-base were not used when initializing HubertModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of HubertModel were not initialized from the model checkpoint at GPT_SoVITS/pretrained_models/chinese-hubert-base and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "<All keys matched successfully>\n",
            "Number of parameter: 77.49M\n",
            "IMPORTANT: You are using gradio version 3.38.0, however version 4.29.0 is available, please upgrade.\n",
            "--------\n",
            "Running on local URL:  http://0.0.0.0:9872\n",
            "Running on public URL: https://727c9aac5850d67f18.gradio.live\n",
            "\"/usr/bin/python3\" tools/subfix_webui.py --load_list \"output/asr_opt/slicer_opt.list\" --webui_port 9871 --is_share False\n",
            "/content/GPT-SoVITS-v2/tools/subfix_webui.py:366: GradioUnusedKwargWarning: You have unused kwarg parameters in Button, please remove them: {'link': '?__theme=light'}\n",
            "  btn_theme_dark = gr.Button(\"Light Theme\", link=\"?__theme=light\", scale=1)\n",
            "/content/GPT-SoVITS-v2/tools/subfix_webui.py:367: GradioUnusedKwargWarning: You have unused kwarg parameters in Button, please remove them: {'link': '?__theme=dark'}\n",
            "  btn_theme_light = gr.Button(\"Dark Theme\", link=\"?__theme=dark\", scale=1)\n",
            "Running on local URL:  http://0.0.0.0:9871\n",
            "IMPORTANT: You are using gradio version 3.38.0, however version 4.29.0 is available, please upgrade.\n",
            "--------\n",
            "Running on public URL: https://3016e0629888f8809e.gradio.live\n",
            "实际输入的参考文本: 以本君推断此次考核的内容。\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/routes.py\", line 442, in run_predict\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1389, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1108, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 346, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 339, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 322, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 691, in gen_wrapper\n",
            "    yield from f(*args, **kwargs)\n",
            "  File \"/content/GPT-SoVITS-v2/GPT_SoVITS/inference_webui.py\", line 325, in get_tts_wav\n",
            "    if (text[0] not in splits and len(get_first(text)) < 4): text = \"。\" + text if text_language != \"en\" else \".\" + text\n",
            "IndexError: string index out of range\n",
            "实际输入的参考文本: 以本君推断此次考核的内容。\n",
            "实际输入的目标文本: 我最喜欢钱儿了\n",
            "实际输入的目标文本(切句后): 我最喜欢钱儿了\n",
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba_fast:Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /content/GPT-SoVITS-v2/TEMP/jieba.cache\n",
            "DEBUG:jieba_fast:Loading model from cache /content/GPT-SoVITS-v2/TEMP/jieba.cache\n",
            "Loading model cost 1.255 seconds.\n",
            "DEBUG:jieba_fast:Loading model cost 1.255 seconds.\n",
            "Prefix dict has been built succesfully.\n",
            "DEBUG:jieba_fast:Prefix dict has been built succesfully.\n",
            "实际输入的目标文本(每句): 我最喜欢钱儿了。\n",
            "前端处理后的文本(每句): 我最喜欢钱儿了.\n",
            "  0% 1/1500 [00:00<09:13,  2.71it/s]T2S Decoding EOS [108 -> 110]\n",
            "  0% 1/1500 [00:00<11:20,  2.20it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:665: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
            "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:873.)\n",
            "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
            "3.796\t1.411\t0.480\t1.271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l28zMG05rKoJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}